Of course. Here is a comprehensive, in-depth engineering notebook for your CFB Pick'em Model, structured to be clear, thorough, and serve as an excellent technical guide for the project.

-----

# CFB Pick’em Model — Engineering Notebook

This project is a complete, automated data and machine learning pipeline for predicting weekly NCAA college football games. It is designed for transparency, reproducibility, and robust performance by leveraging a multi-faceted feature set, a carefully designed validation strategy, and an ensemble of complementary models.

The entire system is orchestrated via **GitHub Actions** and serves its predictions through a static **GitHub Pages** website.

-----

## 1\. High-Level Architecture

The system is designed as a three-stage pipeline that runs automatically, transforming raw data into calibrated game predictions.

1.  **Data Ingestion (`build_dataset.py`)**: This stage is responsible for loading all historical data (schedules, team stats, betting lines, etc.), cleaning it, and engineering a rich feature set. The output is a clean, wide-format `training.parquet` file where each row represents a single game with all its associated features and the final outcome.
2.  **Model Training (`train_model.py`)**: This stage consumes the `training.parquet` file. It trains a gradient boosting model using a "season-ahead" cross-validation strategy to prevent data leakage. The model is then calibrated to ensure its predicted probabilities are reliable. The final trained model is saved as `model.joblib`.
3.  **Prediction (`predict.py`)**: This stage loads the trained model and applies the *exact same* feature engineering logic to the user-provided weekly matchups from `docs/input/games.txt`. It generates a `predictions.json` file containing the pick and win probability for each game.

This modular, three-script design ensures a clean separation of concerns, making the system easier to debug, maintain, and extend.

-----

## 2\. Repository Layout

The repository is organized to separate code, data, user inputs, and website assets.

```
/scripts
  /lib                     # Shared utility functions (the "how")
    __init__.py
    context.py             # Rest, travel, and game context features
    elo.py                 # Elo rating calculations
    io_utils.py            # Data loading and saving helpers
    market.py              # Betting market data processing
    parsing.py             # Parsing user input and cleaning data
    rolling.py             # Rolling average feature calculations
  __init__.py
  build_dataset.py         # The "what": Stage 1 - Data ingestion and feature engineering
  train_model.py           # The "what": Stage 2 - Model training and validation
  predict.py               # The "what": Stage 3 - Prediction generation
  requirements.txt         # Python dependencies

/data
  /raw/cfbd                # Raw CSVs from CollegeFootballData API (optional local cache)
  /derived                 # Intermediate and final model artifacts
    training.parquet       # The clean, final feature set for model training
    model.joblib           # The final, trained, and calibrated model object

/docs                      # Root folder for the GitHub Pages website
  /input                   # Files you edit to control the model
    games.txt              # Your weekly matchups (one per line)
    aliases.json           # (Optional) Team name aliases
    lines.csv              # (Optional) Manual betting lines for upcoming games
  /data                    # Files automatically generated by the workflow
    predictions.json       # The final output read by the website
    train_meta.json        # Metadata from training (feature lists, market params)
    train_metrics.json     # Model performance metrics (AUC, Brier score)
  index.html               # The website's main page
  app.js                   # JavaScript to fetch predictions.json and render the UI

/.github/workflows
  predict.yml              # The GitHub Actions workflow that orchestrates the entire pipeline
```

-----

## 3\. Feature Engineering

The model's performance relies on a set of carefully engineered features designed to capture different aspects of a team's strength and the context of a specific game.

### 3.1 Rolling Form Statistics

Instead of using season-long averages, which can be misleading, we use a team's **recent form**.

  * **Logic**: We calculate rolling averages for key statistical categories (e.g., `ppa`, `success_rate`, `turnovers`) over a team's **last 5 games**.
  * **Side-Awareness**: Crucially, these rolling averages are **side-aware**. The home team's form is calculated from its last 5 **home** games, and the away team's form is from its last 5 **away** games. This captures performance differences that arise from playing in familiar vs. hostile environments.
  * **Leakage Prevention**: All rolling averages are calculated using a `.shift(1)` operation to ensure a game's own stats are never included in the pre-game features.
  * **Differentials**: The final features fed to the model are the **differences** between the home and away team's rolling stats (e.g., `diff_R5_ppa = home_R5_ppa - away_R5_ppa`).

### 3.2 Game Context Features

This group of features accounts for the logistical and situational factors of a game.

  * **Rest**: We calculate the number of days since each team's last game and compute the difference (`rest_diff`). We also generate flags for short weeks and bye weeks.
  * **Travel**: Using the haversine formula on latitude/longitude coordinates of team campuses and stadiums, we calculate the travel distance in kilometers for the away team (`travel_away_km`). Home team travel is assumed to be zero.
  * **Flags**: Simple binary flags for `neutral_site` and `is_postseason` games.

### 3.3 Market-Derived Features

The betting market is an incredibly efficient signal. We incorporate it in two ways:

  * **Median Lines**: We process historical betting lines data, taking the median `spread` and `over_under` for each game to create stable market indicators.
  * **Market-Implied Probability**: We don't use the spread directly. Instead, we train a logistic function that maps the historical point spread to the actual win/loss outcome. This gives us a learned function `p(win) = f(spread)`. For any game with a spread, we can then calculate a `market_home_prob`, a powerful predictive feature.

### 3.4 Elo Rating

Elo provides a long-term, continuously updated power rating for each team.

  * **Core Logic**: A standard Elo system where teams exchange points based on the game's outcome and the pre-game expected outcome.
  * **Enhancements**: Our Elo model includes a **home-field advantage** (HFA) of \~65 points (nullified on neutral sites) and **off-season regression to the mean** to account for roster turnover.
  * **As a Feature**: The final Elo win probability (`elo_home_prob`) is not used as the final prediction but rather as another input feature for the main model to consider.

-----

## 4\. Model Training and Validation

The modeling stage is designed for robustness and to produce reliable, well-calibrated probabilities.

### 4.1 Model Choice

We use a `HistGradientBoostingClassifier` from scikit-learn. This is a modern, fast, and powerful tree-based model that can effectively capture non-linear relationships between the features.

### 4.2 Season-Ahead Validation

To get a realistic estimate of out-of-sample performance and prevent data leakage, we use a **season-ahead validation** strategy. For each season `S` in our history, we:

1.  **Train** the model on all seasons *before* `S`.
2.  **Test** the model on season `S`.

We then average the performance metrics (AUC, Brier score, accuracy) across all tested seasons. This mimics the real-world task of predicting a future season using only past data.

### 4.3 Probability Calibration

The raw output of a gradient boosting model can be poorly calibrated (e.g., when it predicts 70%, the event might only happen 60% of the time). We fix this using `CalibratedClassifierCV`.

  * **Logic**: After the main model is trained, a second model (an isotonic regression) is trained to correct the probability scores. It learns a mapping from the model's raw predicted probabilities to the true historical frequencies.
  * **Result**: This ensures that when the final model predicts a 70% chance of winning, that team does, in fact, win approximately 70% of the time over the long run. This is measured by the **Brier score**.

-----

## 5\. Automation with GitHub Actions

The entire pipeline is orchestrated by a single workflow file at `.github/workflows/predict.yml`.

  * **Triggers**: The workflow runs automatically:
      * **Hourly**: To re-train the model and generate new predictions.
      * **On Push**: Whenever code or input files are changed.
      * **Manually**: Via the "Run workflow" button on the Actions tab.
  * **Permissions**: The workflow is granted `contents: write` permissions to allow it to commit the generated artifacts (`predictions.json`, `model.joblib`, etc.) back to the repository.
  * **Commit Strategy**: To prevent conflicts from concurrent runs, the workflow pushes its results to a dedicated **`bot/predictions`** branch. This cleanly separates automated commits from your development work on the `main` branch. The GitHub Pages site is configured to serve from this `bot/predictions` branch, ensuring it always displays the latest results.

-----

## 6\. How to Use and Extend

  * **Weekly Use**: Simply update `docs/input/games.txt` with the new week's matchups and push the change. The workflow will handle the rest.
  * **Extending**: The modular `scripts/lib` structure makes it easy to add new features. For example, to add weather data, you would:
    1.  Source historical weather data.
    2.  Add a `weather.py` file to `scripts/lib` with a function to calculate weather features for a game.
    3.  Call this new function in `build_dataset.py` and `predict.py` and add the new feature names to the model's feature list.
